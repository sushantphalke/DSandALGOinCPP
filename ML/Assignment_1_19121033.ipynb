{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### ASSIGNMENT NO 1\n",
    "#### To Predict the price of the Uber ride from a given pickup point to the agreed drop-off location. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-19T19:40:07.277098Z",
     "iopub.status.busy": "2022-04-19T19:40:07.276527Z",
     "iopub.status.idle": "2022-04-19T19:40:08.95391Z",
     "shell.execute_reply": "2022-04-19T19:40:08.95303Z",
     "shell.execute_reply.started": "2022-04-19T19:40:07.27697Z"
    }
   },
   "outputs": [],
   "source": [
    "#Importing the basic librarires\n",
    "\n",
    "import os\n",
    "import math\n",
    "import scipy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import datetime as dt\n",
    "import geopy.distance\n",
    "from tqdm import tqdm\n",
    "from IPython.display import display\n",
    "\n",
    "from statsmodels.formula import api\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [10,6]\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-01T16:20:11.502891Z",
     "iopub.status.busy": "2022-01-01T16:20:11.502586Z",
     "iopub.status.idle": "2022-01-01T16:20:12.12217Z",
     "shell.execute_reply": "2022-01-01T16:20:12.121372Z",
     "shell.execute_reply.started": "2022-01-01T16:20:11.502851Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>passenger_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.5</td>\n",
       "      <td>2015-05-07 19:52:06 UTC</td>\n",
       "      <td>-73.999817</td>\n",
       "      <td>40.738354</td>\n",
       "      <td>-73.999512</td>\n",
       "      <td>40.723217</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.7</td>\n",
       "      <td>2009-07-17 20:04:56 UTC</td>\n",
       "      <td>-73.994355</td>\n",
       "      <td>40.728225</td>\n",
       "      <td>-73.994710</td>\n",
       "      <td>40.750325</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12.9</td>\n",
       "      <td>2009-08-24 21:45:00 UTC</td>\n",
       "      <td>-74.005043</td>\n",
       "      <td>40.740770</td>\n",
       "      <td>-73.962565</td>\n",
       "      <td>40.772647</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.3</td>\n",
       "      <td>2009-06-26 08:22:21 UTC</td>\n",
       "      <td>-73.976124</td>\n",
       "      <td>40.790844</td>\n",
       "      <td>-73.965316</td>\n",
       "      <td>40.803349</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16.0</td>\n",
       "      <td>2014-08-28 17:47:00 UTC</td>\n",
       "      <td>-73.925023</td>\n",
       "      <td>40.744085</td>\n",
       "      <td>-73.973082</td>\n",
       "      <td>40.761247</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fare_amount          pickup_datetime  pickup_longitude  pickup_latitude  \\\n",
       "0          7.5  2015-05-07 19:52:06 UTC        -73.999817        40.738354   \n",
       "1          7.7  2009-07-17 20:04:56 UTC        -73.994355        40.728225   \n",
       "2         12.9  2009-08-24 21:45:00 UTC        -74.005043        40.740770   \n",
       "3          5.3  2009-06-26 08:22:21 UTC        -73.976124        40.790844   \n",
       "4         16.0  2014-08-28 17:47:00 UTC        -73.925023        40.744085   \n",
       "\n",
       "   dropoff_longitude  dropoff_latitude  passenger_count  \n",
       "0         -73.999512         40.723217                1  \n",
       "1         -73.994710         40.750325                1  \n",
       "2         -73.962565         40.772647                1  \n",
       "3         -73.965316         40.803349                3  \n",
       "4         -73.973082         40.761247                5  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1mInference:\u001b[0m The Datset consists of 7 features & 200000 samples.\n"
     ]
    }
   ],
   "source": [
    "#Importing the dataset\n",
    "\n",
    "df = pd.read_csv('C:/Users/DELL/BE Computer/ML_Submission_19121033/ML_Submission_19121033/csvs/uber.csv')\n",
    "\n",
    "df.drop(['Unnamed: 0','key'], axis=1, inplace=True)\n",
    "display(df.head())\n",
    "\n",
    "target = 'fare_amount'\n",
    "features = [i for i in df.columns if i not in [target]]\n",
    "\n",
    "print('\\n\\033[1mInference:\\033[0m The Datset consists of {} features & {} samples.'.format(df.shape[1], df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-01T16:20:12.123729Z",
     "iopub.status.busy": "2022-01-01T16:20:12.123505Z",
     "iopub.status.idle": "2022-01-01T16:20:12.197098Z",
     "shell.execute_reply": "2022-01-01T16:20:12.196262Z",
     "shell.execute_reply.started": "2022-01-01T16:20:12.1237Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Total Null Values  Percentage\n",
      "fare_amount                        0         0.0\n",
      "pickup_datetime                    0         0.0\n",
      "pickup_longitude                   0         0.0\n",
      "pickup_latitude                    0         0.0\n",
      "passenger_count                    0         0.0\n",
      "dropoff_longitude                  1         0.0\n",
      "dropoff_latitude                   1         0.0\n"
     ]
    }
   ],
   "source": [
    "#Check for empty elements\n",
    "\n",
    "nvc = pd.DataFrame(df.isnull().sum().sort_values(), columns=['Total Null Values'])\n",
    "nvc['Percentage'] = round(nvc['Total Null Values']/df.shape[0],3)*100\n",
    "print(nvc)\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-01T16:20:12.199375Z",
     "iopub.status.busy": "2022-01-01T16:20:12.198422Z",
     "iopub.status.idle": "2022-01-01T16:21:46.206966Z",
     "shell.execute_reply": "2022-01-01T16:21:46.206007Z",
     "shell.execute_reply.started": "2022-01-01T16:20:12.199328Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>year</th>\n",
       "      <th>weekday</th>\n",
       "      <th>Monthly_Quarter</th>\n",
       "      <th>Hourly_Segments</th>\n",
       "      <th>Distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.5</td>\n",
       "      <td>-73.999817</td>\n",
       "      <td>40.738354</td>\n",
       "      <td>-73.999512</td>\n",
       "      <td>40.723217</td>\n",
       "      <td>1</td>\n",
       "      <td>2015</td>\n",
       "      <td>3</td>\n",
       "      <td>Q2</td>\n",
       "      <td>H5</td>\n",
       "      <td>1681.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.7</td>\n",
       "      <td>-73.994355</td>\n",
       "      <td>40.728225</td>\n",
       "      <td>-73.994710</td>\n",
       "      <td>40.750325</td>\n",
       "      <td>1</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>Q3</td>\n",
       "      <td>H6</td>\n",
       "      <td>2454.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12.9</td>\n",
       "      <td>-74.005043</td>\n",
       "      <td>40.740770</td>\n",
       "      <td>-73.962565</td>\n",
       "      <td>40.772647</td>\n",
       "      <td>1</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>Q3</td>\n",
       "      <td>H6</td>\n",
       "      <td>5039.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.3</td>\n",
       "      <td>-73.976124</td>\n",
       "      <td>40.790844</td>\n",
       "      <td>-73.965316</td>\n",
       "      <td>40.803349</td>\n",
       "      <td>3</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>Q2</td>\n",
       "      <td>H3</td>\n",
       "      <td>1661.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16.0</td>\n",
       "      <td>-73.925023</td>\n",
       "      <td>40.744085</td>\n",
       "      <td>-73.973082</td>\n",
       "      <td>40.761247</td>\n",
       "      <td>5</td>\n",
       "      <td>2014</td>\n",
       "      <td>3</td>\n",
       "      <td>Q3</td>\n",
       "      <td>H5</td>\n",
       "      <td>4483.73</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fare_amount  pickup_longitude  pickup_latitude  dropoff_longitude  \\\n",
       "0          7.5        -73.999817        40.738354         -73.999512   \n",
       "1          7.7        -73.994355        40.728225         -73.994710   \n",
       "2         12.9        -74.005043        40.740770         -73.962565   \n",
       "3          5.3        -73.976124        40.790844         -73.965316   \n",
       "4         16.0        -73.925023        40.744085         -73.973082   \n",
       "\n",
       "   dropoff_latitude  passenger_count  year  weekday Monthly_Quarter  \\\n",
       "0         40.723217                1  2015        3              Q2   \n",
       "1         40.750325                1  2009        4              Q3   \n",
       "2         40.772647                1  2009        0              Q3   \n",
       "3         40.803349                3  2009        4              Q2   \n",
       "4         40.761247                5  2014        3              Q3   \n",
       "\n",
       "  Hourly_Segments  Distance  \n",
       "0              H5   1681.11  \n",
       "1              H6   2454.36  \n",
       "2              H6   5039.60  \n",
       "3              H3   1661.44  \n",
       "4              H5   4483.73  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reframing the columns\n",
    "\n",
    "df = df[(df.pickup_latitude<90) & (df.dropoff_latitude<90) &\n",
    "        (df.pickup_latitude>-90) & (df.dropoff_latitude>-90) &\n",
    "        (df.pickup_longitude<180) & (df.dropoff_longitude<180) &\n",
    "        (df.pickup_longitude>-180) & (df.dropoff_longitude>-180)]\n",
    "\n",
    "df.pickup_datetime=pd.to_datetime(df.pickup_datetime)\n",
    "\n",
    "df['year'] = df.pickup_datetime.dt.year\n",
    "df['month'] = df.pickup_datetime.dt.month\n",
    "df['weekday'] = df.pickup_datetime.dt.weekday\n",
    "df['hour'] = df.pickup_datetime.dt.hour\n",
    "\n",
    "df['Monthly_Quarter'] = df.month.map({1:'Q1',2:'Q1',3:'Q1',4:'Q2',5:'Q2',6:'Q2',7:'Q3',\n",
    "                                      8:'Q3',9:'Q3',10:'Q4',11:'Q4',12:'Q4'})\n",
    "df['Hourly_Segments'] = df.hour.map({0:'H1',1:'H1',2:'H1',3:'H1',4:'H2',5:'H2',6:'H2',7:'H2',8:'H3',\n",
    "                                     9:'H3',10:'H3',11:'H3',12:'H4',13:'H4',14:'H4',15:'H4',16:'H5',\n",
    "                                     17:'H5',18:'H5',19:'H5',20:'H6',21:'H6',22:'H6',23:'H6'})\n",
    "\n",
    "df['Distance']=[round(geopy.distance.distance((df.pickup_latitude[i], df.pickup_longitude[i]),(df.dropoff_latitude[i], df.dropoff_longitude[i])).m,2) for i in df.index]\n",
    "\n",
    "df.drop(['pickup_datetime','month', 'hour',], axis=1, inplace=True)\n",
    "\n",
    "original_df = df.copy(deep=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-01T16:21:46.210132Z",
     "iopub.status.busy": "2022-01-01T16:21:46.209623Z",
     "iopub.status.idle": "2022-01-01T16:21:46.282014Z",
     "shell.execute_reply": "2022-01-01T16:21:46.28097Z",
     "shell.execute_reply.started": "2022-01-01T16:21:46.210083Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-01T16:21:46.283527Z",
     "iopub.status.busy": "2022-01-01T16:21:46.283274Z",
     "iopub.status.idle": "2022-01-01T16:21:46.375886Z",
     "shell.execute_reply": "2022-01-01T16:21:46.375274Z",
     "shell.execute_reply.started": "2022-01-01T16:21:46.283498Z"
    }
   },
   "outputs": [],
   "source": [
    "df.nunique().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-01T16:21:46.377349Z",
     "iopub.status.busy": "2022-01-01T16:21:46.377004Z",
     "iopub.status.idle": "2022-01-01T16:21:46.473912Z",
     "shell.execute_reply": "2022-01-01T16:21:46.47293Z",
     "shell.execute_reply.started": "2022-01-01T16:21:46.37732Z"
    }
   },
   "outputs": [],
   "source": [
    "nu = df.drop([target], axis=1).nunique().sort_values()\n",
    "nf = []; cf = []; nnf = 0; ncf = 0; #numerical & categorical features\n",
    "\n",
    "for i in range(df.drop([target], axis=1).shape[1]):\n",
    "    if nu.values[i]<=24:cf.append(nu.index[i])\n",
    "    else: nf.append(nu.index[i])\n",
    "\n",
    "print('\\n\\033[1mInference:\\033[0m The Datset has {} numerical & {} categorical features.'.format(len(nf),len(cf)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-01T16:21:46.475443Z",
     "iopub.status.busy": "2022-01-01T16:21:46.475106Z",
     "iopub.status.idle": "2022-01-01T16:21:46.574481Z",
     "shell.execute_reply": "2022-01-01T16:21:46.573605Z",
     "shell.execute_reply.started": "2022-01-01T16:21:46.47541Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>year</th>\n",
       "      <th>weekday</th>\n",
       "      <th>Distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>199987.000000</td>\n",
       "      <td>199987.000000</td>\n",
       "      <td>199987.000000</td>\n",
       "      <td>199987.000000</td>\n",
       "      <td>199987.000000</td>\n",
       "      <td>199987.000000</td>\n",
       "      <td>199987.000000</td>\n",
       "      <td>199987.000000</td>\n",
       "      <td>1.999870e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>11.359849</td>\n",
       "      <td>-72.501786</td>\n",
       "      <td>39.917937</td>\n",
       "      <td>-72.511608</td>\n",
       "      <td>39.922031</td>\n",
       "      <td>1.684544</td>\n",
       "      <td>2011.742463</td>\n",
       "      <td>3.048383</td>\n",
       "      <td>2.056346e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>9.901868</td>\n",
       "      <td>10.449955</td>\n",
       "      <td>6.130412</td>\n",
       "      <td>10.412192</td>\n",
       "      <td>6.117669</td>\n",
       "      <td>1.385999</td>\n",
       "      <td>1.856438</td>\n",
       "      <td>1.946960</td>\n",
       "      <td>3.796638e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-52.000000</td>\n",
       "      <td>-93.824668</td>\n",
       "      <td>-74.015515</td>\n",
       "      <td>-75.458979</td>\n",
       "      <td>-74.015750</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2009.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>-73.992064</td>\n",
       "      <td>40.734793</td>\n",
       "      <td>-73.991407</td>\n",
       "      <td>40.733823</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2010.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.215530e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>8.500000</td>\n",
       "      <td>-73.981822</td>\n",
       "      <td>40.752592</td>\n",
       "      <td>-73.980092</td>\n",
       "      <td>40.753042</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2012.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.121280e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>12.500000</td>\n",
       "      <td>-73.967154</td>\n",
       "      <td>40.767157</td>\n",
       "      <td>-73.963658</td>\n",
       "      <td>40.768000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2013.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>3.874255e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>499.000000</td>\n",
       "      <td>40.808425</td>\n",
       "      <td>48.018760</td>\n",
       "      <td>40.831932</td>\n",
       "      <td>45.031598</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>2015.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>8.783594e+06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         fare_amount  pickup_longitude  pickup_latitude  dropoff_longitude  \\\n",
       "count  199987.000000     199987.000000    199987.000000      199987.000000   \n",
       "mean       11.359849        -72.501786        39.917937         -72.511608   \n",
       "std         9.901868         10.449955         6.130412          10.412192   \n",
       "min       -52.000000        -93.824668       -74.015515         -75.458979   \n",
       "25%         6.000000        -73.992064        40.734793         -73.991407   \n",
       "50%         8.500000        -73.981822        40.752592         -73.980092   \n",
       "75%        12.500000        -73.967154        40.767157         -73.963658   \n",
       "max       499.000000         40.808425        48.018760          40.831932   \n",
       "\n",
       "       dropoff_latitude  passenger_count           year        weekday  \\\n",
       "count     199987.000000    199987.000000  199987.000000  199987.000000   \n",
       "mean          39.922031         1.684544    2011.742463       3.048383   \n",
       "std            6.117669         1.385999       1.856438       1.946960   \n",
       "min          -74.015750         0.000000    2009.000000       0.000000   \n",
       "25%           40.733823         1.000000    2010.000000       1.000000   \n",
       "50%           40.753042         1.000000    2012.000000       3.000000   \n",
       "75%           40.768000         2.000000    2013.000000       5.000000   \n",
       "max           45.031598       208.000000    2015.000000       6.000000   \n",
       "\n",
       "           Distance  \n",
       "count  1.999870e+05  \n",
       "mean   2.056346e+04  \n",
       "std    3.796638e+05  \n",
       "min    0.000000e+00  \n",
       "25%    1.215530e+03  \n",
       "50%    2.121280e+03  \n",
       "75%    3.874255e+03  \n",
       "max    8.783594e+06  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inference:** The stats seem to be fine, lets do further analysis on the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>  Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-01T16:29:03.24918Z",
     "iopub.status.busy": "2022-01-01T16:29:03.248826Z",
     "iopub.status.idle": "2022-01-01T16:29:04.369659Z",
     "shell.execute_reply": "2022-01-01T16:29:04.369022Z",
     "shell.execute_reply.started": "2022-01-01T16:29:03.249142Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=[15,10])\n",
    "a=plt.imread('https://raw.githubusercontent.com/Masterx-AI/Project_Uber_Fare_Prediction/main/wm.png')\n",
    "plt.imshow(a, alpha=0.2)\n",
    "plt.scatter( (df.pickup_longitude+180)*3,(df.pickup_latitude+215)*1.45555555,alpha=0.3, color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-01T16:30:10.167314Z",
     "iopub.status.busy": "2022-01-01T16:30:10.166412Z",
     "iopub.status.idle": "2022-01-01T16:30:11.175759Z",
     "shell.execute_reply": "2022-01-01T16:30:11.174933Z",
     "shell.execute_reply.started": "2022-01-01T16:30:10.167254Z"
    }
   },
   "outputs": [],
   "source": [
    "#Let us first analyze the distribution of the target variable\n",
    "\n",
    "plt.figure(figsize=[8,4])\n",
    "sns.distplot(df[target], color='g',hist_kws=dict(edgecolor=\"black\", linewidth=2), bins=30)\n",
    "plt.title('Target Variable Distribution - Median Value of Homes ($1Ms)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inference:**The Target Variable seems to be be highly skewed, with most datapoints lieing near 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-01T16:30:11.276549Z",
     "iopub.status.busy": "2022-01-01T16:30:11.275846Z",
     "iopub.status.idle": "2022-01-01T16:30:12.607178Z",
     "shell.execute_reply": "2022-01-01T16:30:12.604931Z",
     "shell.execute_reply.started": "2022-01-01T16:30:11.276498Z"
    }
   },
   "outputs": [],
   "source": [
    "#Visualising the categorical features \n",
    "\n",
    "print('\\033[1mVisualising Categorical Features:'.center(100))\n",
    "\n",
    "n=2\n",
    "plt.figure(figsize=[15,3*math.ceil(len(cf)/n)])\n",
    "\n",
    "\n",
    "for i in range(len(cf)):\n",
    "    if df[cf[i]].nunique()<=12:\n",
    "        plt.subplot(math.ceil(len(cf)/n),n,i+1)\n",
    "        sns.countplot(df[cf[i]])\n",
    "    else:\n",
    "        plt.subplot(3,1,i-3)\n",
    "        sns.countplot(df[cf[i]])\n",
    "       \n",
    "        \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inference:** The categorical features distribution can be seen in the above plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-01T16:30:22.112367Z",
     "iopub.status.busy": "2022-01-01T16:30:22.111923Z",
     "iopub.status.idle": "2022-01-01T16:30:28.740359Z",
     "shell.execute_reply": "2022-01-01T16:30:28.739508Z",
     "shell.execute_reply.started": "2022-01-01T16:30:22.112321Z"
    }
   },
   "outputs": [],
   "source": [
    "#Visualising the numeric features \n",
    "\n",
    "print('\\033[1mNumeric Features Distribution'.center(100))\n",
    "\n",
    "n=5\n",
    "\n",
    "plt.figure(figsize=[15,5*math.ceil(len(nf)/n)])\n",
    "for i in range(len(nf)):\n",
    "    plt.subplot(math.ceil(len(nf)/3),n,i+1)\n",
    "    sns.distplot(df[nf[i]],hist_kws=dict(edgecolor=\"black\", linewidth=2), bins=10, color=list(np.random.randint([255,255,255])/255))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=[15,5*math.ceil(len(nf)/n)])\n",
    "for i in range(len(nf)):\n",
    "    plt.subplot(math.ceil(len(nf)/3),n,i+1)\n",
    "    df.boxplot(nf[i])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>  Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-01T16:30:28.748411Z",
     "iopub.status.busy": "2022-01-01T16:30:28.748092Z",
     "iopub.status.idle": "2022-01-01T16:30:28.91047Z",
     "shell.execute_reply": "2022-01-01T16:30:28.909604Z",
     "shell.execute_reply.started": "2022-01-01T16:30:28.748372Z"
    }
   },
   "outputs": [],
   "source": [
    "#Removal of any Duplicate rows (if any)\n",
    "\n",
    "counter = 0\n",
    "rs,cs = original_df.shape\n",
    "\n",
    "df.drop_duplicates(inplace=True)\n",
    "df.drop(['pickup_latitude','pickup_longitude',\n",
    "         'dropoff_latitude','dropoff_longitude'],axis=1)\n",
    "\n",
    "if df.shape==(rs,cs):\n",
    "    print('\\n\\033[1mInference:\\033[0m The dataset doesn\\'t have any duplicates')\n",
    "else:\n",
    "    print(f'\\n\\033[1mInference:\\033[0m Number of duplicates dropped/fixed ---> {rs-df.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-02T10:06:00.633656Z",
     "iopub.status.busy": "2022-01-02T10:06:00.633206Z",
     "iopub.status.idle": "2022-01-02T10:06:00.655444Z",
     "shell.execute_reply": "2022-01-02T10:06:00.654732Z",
     "shell.execute_reply.started": "2022-01-02T10:06:00.633534Z"
    }
   },
   "outputs": [],
   "source": [
    "# Since only one pair of values are missing in the dataset, we can just drop them\n",
    "\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-01T16:30:28.917647Z",
     "iopub.status.busy": "2022-01-01T16:30:28.917439Z",
     "iopub.status.idle": "2022-01-01T16:30:29.236103Z",
     "shell.execute_reply": "2022-01-01T16:30:29.235274Z",
     "shell.execute_reply.started": "2022-01-01T16:30:28.917622Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Converting categorical Columns to Numeric\n",
    "\n",
    "df1 = df.copy()\n",
    "df3 = df1.copy()\n",
    "\n",
    "ecc = nvc[nvc['Percentage']!=0].index.values\n",
    "fcc = [i for i in cf if i not in ecc]\n",
    "#One-Hot Binay Encoding\n",
    "oh=True\n",
    "dm=True\n",
    "for i in fcc:\n",
    "    #print(i)\n",
    "    if df3[i].nunique()==2:\n",
    "        if oh==True: print(\"\\033[1mOne-Hot Encoding on features:\\033[0m\")\n",
    "        print(i);oh=False\n",
    "        df3[i]=pd.get_dummies(df3[i], drop_first=True, prefix=str(i))\n",
    "    if (df3[i].nunique()>2 and df3[i].nunique()<17):\n",
    "        if dm==True: print(\"\\n\\033[1mDummy Encoding on features:\\033[0m\")\n",
    "        print(i);dm=False\n",
    "        df3 = pd.concat([df3.drop([i], axis=1), pd.DataFrame(pd.get_dummies(df3[i], drop_first=True, prefix=str(i)))],axis=1)\n",
    "        \n",
    "df3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-01T16:30:29.237933Z",
     "iopub.status.busy": "2022-01-01T16:30:29.237432Z",
     "iopub.status.idle": "2022-01-01T16:30:29.427804Z",
     "shell.execute_reply": "2022-01-01T16:30:29.427023Z",
     "shell.execute_reply.started": "2022-01-01T16:30:29.237898Z"
    }
   },
   "outputs": [],
   "source": [
    "#Removal of outlier:\n",
    "\n",
    "df1 = df3.copy()\n",
    "\n",
    "#features1 = [i for i in features if i not in ['CHAS','RAD']]\n",
    "features1 = nf\n",
    "\n",
    "for i in features1:\n",
    "    Q1 = df1[i].quantile(0.25)\n",
    "    Q3 = df1[i].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    df1 = df1[df1[i] <= (Q3+(1.5*IQR))]\n",
    "    df1 = df1[df1[i] >= (Q1-(1.5*IQR))]\n",
    "    df1 = df1.reset_index(drop=True)\n",
    "display(df1.head())\n",
    "print('\\n\\033[1mInference:\\033[0m\\nBefore removal of outliers, The dataset had {} samples.'.format(df3.shape[0]))\n",
    "print('After removal of outliers, The dataset now has {} samples.'.format(df1.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-01T16:30:29.429Z",
     "iopub.status.busy": "2022-01-01T16:30:29.428783Z",
     "iopub.status.idle": "2022-01-01T16:30:29.57081Z",
     "shell.execute_reply": "2022-01-01T16:30:29.570262Z",
     "shell.execute_reply.started": "2022-01-01T16:30:29.428974Z"
    }
   },
   "outputs": [],
   "source": [
    "#Final Dataset size after performing Preprocessing\n",
    "\n",
    "df = df1.copy()\n",
    "df.columns=[i.replace('-','_') for i in df.columns]\n",
    "\n",
    "plt.title('Final Dataset')\n",
    "plt.pie([df.shape[0], original_df.shape[0]-df.shape[0]], radius = 1, labels=['Retained','Dropped'], counterclock=False, \n",
    "        autopct='%1.1f%%', pctdistance=0.9, explode=[0,0], shadow=True)\n",
    "plt.pie([df.shape[0]], labels=['100%'], labeldistance=-0, radius=0.78)\n",
    "plt.show()\n",
    "\n",
    "print(f'\\n\\033[1mInference:\\033[0m After the cleanup process, {original_df.shape[0]-df.shape[0]} samples were dropped, \\\n",
    "while retaining {round(100 - (df.shape[0]*100/(original_df.shape[0])),2)}% of the data.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>  Data Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-01T16:30:29.572318Z",
     "iopub.status.busy": "2022-01-01T16:30:29.572085Z",
     "iopub.status.idle": "2022-01-01T16:30:29.61402Z",
     "shell.execute_reply": "2022-01-01T16:30:29.61306Z",
     "shell.execute_reply.started": "2022-01-01T16:30:29.572267Z"
    }
   },
   "outputs": [],
   "source": [
    "#Splitting the data intro training & testing sets\n",
    "\n",
    "m=[]\n",
    "for i in df.columns.values:\n",
    "    m.append(i.replace(' ','_'))\n",
    "    \n",
    "df.columns = m\n",
    "X = df.drop([target],axis=1)\n",
    "Y = df[target]\n",
    "Train_X, Test_X, Train_Y, Test_Y = train_test_split(X, Y, train_size=0.8, test_size=0.2, random_state=100)\n",
    "Train_X.reset_index(drop=True,inplace=True)\n",
    "\n",
    "print('Original set  ---> ',X.shape,Y.shape,'\\nTraining set  ---> ',Train_X.shape,Train_Y.shape,'\\nTesting set   ---> ', Test_X.shape,'', Test_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-01T16:30:29.616155Z",
     "iopub.status.busy": "2022-01-01T16:30:29.615942Z",
     "iopub.status.idle": "2022-01-01T16:30:30.059956Z",
     "shell.execute_reply": "2022-01-01T16:30:30.059363Z",
     "shell.execute_reply.started": "2022-01-01T16:30:29.616128Z"
    }
   },
   "outputs": [],
   "source": [
    "#Feature Scaling (Standardization)\n",
    "\n",
    "std = StandardScaler()\n",
    "\n",
    "print('\\033[1mStandardardization on Training set'.center(100))\n",
    "Train_X_std = std.fit_transform(Train_X)\n",
    "Train_X_std = pd.DataFrame(Train_X_std, columns=X.columns)\n",
    "display(Train_X_std.describe())\n",
    "\n",
    "print('\\n','\\033[1mStandardardization on Testing set'.center(100))\n",
    "Test_X_std = std.transform(Test_X)\n",
    "Test_X_std = pd.DataFrame(Test_X_std, columns=X.columns)\n",
    "display(Test_X_std.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>  Feature Selection/Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-01T16:30:30.06307Z",
     "iopub.status.busy": "2022-01-01T16:30:30.062835Z",
     "iopub.status.idle": "2022-01-01T16:30:35.570632Z",
     "shell.execute_reply": "2022-01-01T16:30:35.566456Z",
     "shell.execute_reply.started": "2022-01-01T16:30:30.063041Z"
    }
   },
   "outputs": [],
   "source": [
    "#Checking the correlation\n",
    "\n",
    "print('\\033[1mCorrelation Matrix'.center(100))\n",
    "plt.figure(figsize=[24,20])\n",
    "sns.heatmap(df.corr(), annot=True, vmin=-1, vmax=1, center=0) #cmap='BuGn'\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inference:** There seems to be strong multi-correlation between the features. Let us try to fix these..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-01T16:30:35.572244Z",
     "iopub.status.busy": "2022-01-01T16:30:35.571859Z",
     "iopub.status.idle": "2022-01-01T16:30:36.190624Z",
     "shell.execute_reply": "2022-01-01T16:30:36.189787Z",
     "shell.execute_reply.started": "2022-01-01T16:30:35.572205Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Testing a Linear Regression model with statsmodels\n",
    "\n",
    "Train_xy = pd.concat([Train_X,Train_Y.reset_index(drop=True)],axis=1)\n",
    "a = Train_xy.columns.values\n",
    "\n",
    "API = api.ols(formula='{} ~ {}'.format(target,' + '.join(i for i in Train_X.columns)), data=Train_xy).fit()\n",
    "#print(API.conf_int())\n",
    "#print(API.pvalues)\n",
    "API.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5a. Manual Method - VIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-01T16:30:36.192802Z",
     "iopub.status.busy": "2022-01-01T16:30:36.192277Z",
     "iopub.status.idle": "2022-01-01T16:33:57.878775Z",
     "shell.execute_reply": "2022-01-01T16:33:57.877955Z",
     "shell.execute_reply.started": "2022-01-01T16:30:36.19276Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "Trr=[]; Tss=[]; n=3\n",
    "order=['ord-'+str(i) for i in range(2,n)]\n",
    "\n",
    "DROP=[];b=[]\n",
    "\n",
    "for i in tqdm(range(len(Train_X_std.columns)-1)):\n",
    "    vif = pd.DataFrame()\n",
    "    X = Train_X_std.drop(DROP,axis=1)\n",
    "    vif['Features'] = X.columns\n",
    "    vif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "    vif['VIF'] = round(vif['VIF'], 2)\n",
    "    vif = vif.sort_values(by = \"VIF\", ascending = False)\n",
    "    vif.reset_index(drop=True, inplace=True)\n",
    "    if vif.loc[0][1]>=1.1:\n",
    "        DROP.append(vif.loc[0][0])\n",
    "        LR = LinearRegression()\n",
    "        LR.fit(Train_X_std.drop(DROP,axis=1), Train_Y)\n",
    "\n",
    "        pred1 = LR.predict(Train_X_std.drop(DROP,axis=1))\n",
    "        pred2 = LR.predict(Test_X_std.drop(DROP,axis=1))\n",
    "        \n",
    "        Trr.append(np.sqrt(mean_squared_error(Train_Y, pred1)))\n",
    "        Tss.append(np.sqrt(mean_squared_error(Test_Y, pred2)))\n",
    "\n",
    "     \n",
    "print('Dropped Features --> ',DROP)\n",
    "\n",
    "plt.plot(Trr, label='Train RMSE')\n",
    "plt.plot(Tss, label='Test RMSE')\n",
    "#plt.ylim([19.75,20.75])\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5b. Automatic Method - RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-01T16:33:57.880723Z",
     "iopub.status.busy": "2022-01-01T16:33:57.880203Z",
     "iopub.status.idle": "2022-01-01T16:34:53.068974Z",
     "shell.execute_reply": "2022-01-01T16:34:53.067896Z",
     "shell.execute_reply.started": "2022-01-01T16:33:57.880682Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "Trr=[]; Tss=[]; n=3\n",
    "order=['ord-'+str(i) for i in range(2,n)]\n",
    "Trd = pd.DataFrame(np.zeros((10,n-2)), columns=order)\n",
    "Tsd = pd.DataFrame(np.zeros((10,n-2)), columns=order)\n",
    "\n",
    "m=df.shape[1]-2\n",
    "for i in tqdm(range(m)):\n",
    "    lm = LinearRegression()\n",
    "    #lm.fit(Train_X_std, Train_Y)\n",
    "\n",
    "    rfe = RFE(lm,n_features_to_select=Train_X_std.shape[1]-i)             # running RFE\n",
    "    rfe = rfe.fit(Train_X_std, Train_Y)\n",
    "    \n",
    "   \n",
    "    LR = LinearRegression()\n",
    "    LR.fit(Train_X_std.loc[:,rfe.support_], Train_Y)\n",
    "    \n",
    "    #print(Train_X_std.loc[:,rfe.support_].columns)\n",
    "\n",
    "    pred1 = LR.predict(Train_X_std.loc[:,rfe.support_])\n",
    "    pred2 = LR.predict(Test_X_std.loc[:,rfe.support_])\n",
    "\n",
    "    Trr.append(np.sqrt(mean_squared_error(Train_Y, pred1)))\n",
    "    Tss.append(np.sqrt(mean_squared_error(Test_Y, pred2)))\n",
    "\n",
    "\n",
    "plt.plot(Trr, label='Train RMSE')\n",
    "plt.plot(Tss, label='Test RMSE')\n",
    "#plt.ylim([19.75,20.75])\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Elmination using PCA Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-01T16:34:53.071005Z",
     "iopub.status.busy": "2022-01-01T16:34:53.070709Z",
     "iopub.status.idle": "2022-01-01T16:34:53.576426Z",
     "shell.execute_reply": "2022-01-01T16:34:53.575676Z",
     "shell.execute_reply.started": "2022-01-01T16:34:53.070965Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA().fit(Train_X_std)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "x_values = range(1, pca.n_components_+1)\n",
    "ax.bar(x_values, pca.explained_variance_ratio_, lw=2, label='Explained Variance')\n",
    "ax.plot(x_values, np.cumsum(pca.explained_variance_ratio_), lw=2, label='Cumulative Explained Variance', color='red')\n",
    "plt.plot([0,pca.n_components_+1],[0.9,0.9],'g--')\n",
    "ax.set_title('Explained variance of components')\n",
    "ax.set_xlabel('Principal Component')\n",
    "ax.set_ylabel('Explained Variance')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-01T16:34:53.577696Z",
     "iopub.status.busy": "2022-01-01T16:34:53.577482Z",
     "iopub.status.idle": "2022-01-01T16:35:16.728507Z",
     "shell.execute_reply": "2022-01-01T16:35:16.727975Z",
     "shell.execute_reply.started": "2022-01-01T16:34:53.577671Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "Trr=[]; Tss=[]; n=3\n",
    "order=['ord-'+str(i) for i in range(2,n)]\n",
    "Trd = pd.DataFrame(np.zeros((10,n-2)), columns=order)\n",
    "Tsd = pd.DataFrame(np.zeros((10,n-2)), columns=order)\n",
    "m=df.shape[1]-4\n",
    "\n",
    "for i in tqdm(range(m)):\n",
    "    pca = PCA(n_components=Train_X_std.shape[1]-i)\n",
    "    Train_X_std_pca = pca.fit_transform(Train_X_std)\n",
    "    Test_X_std_pca = pca.fit_transform(Test_X_std)\n",
    "    \n",
    "    LR = LinearRegression()\n",
    "    LR.fit(Train_X_std_pca, Train_Y)\n",
    "\n",
    "    pred1 = LR.predict(Train_X_std_pca)\n",
    "    pred2 = LR.predict(Test_X_std_pca)\n",
    "\n",
    "    Trr.append(round(np.sqrt(mean_squared_error(Train_Y, pred1)),2))\n",
    "    Tss.append(round(np.sqrt(mean_squared_error(Test_Y, pred2)),2))\n",
    "\n",
    "\n",
    "\n",
    "plt.plot(Trr, label='Train RMSE')\n",
    "plt.plot(Tss, label='Test RMSE')\n",
    "#plt.ylim([19.5,20.75])\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference:\n",
    "It can be seen that the performance of the modelsis quiet comparable unpon dropping features using VIF, RFE & PCA Techniques. Comparing the RMSE plots, the optimal values were found for dropping most  features using manual RFE Technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-01T16:35:16.730034Z",
     "iopub.status.busy": "2022-01-01T16:35:16.729564Z",
     "iopub.status.idle": "2022-01-01T16:35:19.004369Z",
     "shell.execute_reply": "2022-01-01T16:35:19.003484Z",
     "shell.execute_reply.started": "2022-01-01T16:35:16.730003Z"
    }
   },
   "outputs": [],
   "source": [
    "#Shortlisting the selected Features (with RFE)\n",
    "\n",
    "lm = LinearRegression()\n",
    "rfe = RFE(lm,n_features_to_select=df.shape[1]-23)           \n",
    "rfe = rfe.fit(Train_X_std, Train_Y)\n",
    "\n",
    "LR = LinearRegression()\n",
    "LR.fit(Train_X_std.loc[:,rfe.support_], Train_Y)\n",
    "\n",
    "#print(Train_X_std.loc[:,rfe.support_].columns)\n",
    "\n",
    "pred1 = LR.predict(Train_X_std.loc[:,rfe.support_])\n",
    "pred2 = LR.predict(Test_X_std.loc[:,rfe.support_])\n",
    "\n",
    "print(np.sqrt(mean_squared_error(Train_Y, pred1)))\n",
    "print(np.sqrt(mean_squared_error(Test_Y, pred2)))\n",
    "\n",
    "Train_X_std = Train_X_std.loc[:,rfe.support_]\n",
    "Test_X_std = Test_X_std.loc[:,rfe.support_]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>  Predictive Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-01T16:35:19.006738Z",
     "iopub.status.busy": "2022-01-01T16:35:19.006073Z",
     "iopub.status.idle": "2022-01-01T16:35:19.086254Z",
     "shell.execute_reply": "2022-01-01T16:35:19.085296Z",
     "shell.execute_reply.started": "2022-01-01T16:35:19.00669Z"
    }
   },
   "outputs": [],
   "source": [
    "#Let us first define a function to evaluate our models\n",
    "\n",
    "Model_Evaluation_Comparison_Matrix = pd.DataFrame(np.zeros([5,8]), columns=['Train-R2','Test-R2','Train-RSS','Test-RSS',\n",
    "                                                                            'Train-MSE','Test-MSE','Train-RMSE','Test-RMSE'])\n",
    "rc=np.random.choice(Train_X_std.loc[:,Train_X_std.nunique()>50].columns,3)\n",
    "def Evaluate(n, pred1,pred2):\n",
    "    #Plotting predicted predicteds alongside the actual datapoints \n",
    "    plt.figure(figsize=[15,6])\n",
    "    for e,i in enumerate(rc):\n",
    "        plt.subplot(2,3,e+1)\n",
    "        plt.scatter(y=Train_Y, x=Train_X_std[i], label='Actual')\n",
    "        plt.scatter(y=pred1, x=Train_X_std[i], label='Prediction')\n",
    "        plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    #Evaluating the Multiple Linear Regression Model\n",
    "\n",
    "    print('\\n\\n{}Training Set Metrics{}'.format('-'*20, '-'*20))\n",
    "    print('\\nR2-Score on Training set --->',round(r2_score(Train_Y, pred1),20))\n",
    "    print('Residual Sum of Squares (RSS) on Training set  --->',round(np.sum(np.square(Train_Y-pred1)),20))\n",
    "    print('Mean Squared Error (MSE) on Training set       --->',round(mean_squared_error(Train_Y, pred1),20))\n",
    "    print('Root Mean Squared Error (RMSE) on Training set --->',round(np.sqrt(mean_squared_error(Train_Y, pred1)),20))\n",
    "\n",
    "    print('\\n{}Testing Set Metrics{}'.format('-'*20, '-'*20))\n",
    "    print('\\nR2-Score on Testing set --->',round(r2_score(Test_Y, pred2),20))\n",
    "    print('Residual Sum of Squares (RSS) on Training set  --->',round(np.sum(np.square(Test_Y-pred2)),20))\n",
    "    print('Mean Squared Error (MSE) on Training set       --->',round(mean_squared_error(Test_Y, pred2),20))\n",
    "    print('Root Mean Squared Error (RMSE) on Training set --->',round(np.sqrt(mean_squared_error(Test_Y, pred2)),20))\n",
    "    print('\\n{}Residual Plots{}'.format('-'*20, '-'*20))\n",
    "    \n",
    "    Model_Evaluation_Comparison_Matrix.loc[n,'Train-R2']  = round(r2_score(Train_Y, pred1),20)\n",
    "    Model_Evaluation_Comparison_Matrix.loc[n,'Test-R2']   = round(r2_score(Test_Y, pred2),20)\n",
    "    Model_Evaluation_Comparison_Matrix.loc[n,'Train-RSS'] = round(np.sum(np.square(Train_Y-pred1)),20)\n",
    "    Model_Evaluation_Comparison_Matrix.loc[n,'Test-RSS']  = round(np.sum(np.square(Test_Y-pred2)),20)\n",
    "    Model_Evaluation_Comparison_Matrix.loc[n,'Train-MSE'] = round(mean_squared_error(Train_Y, pred1),20)\n",
    "    Model_Evaluation_Comparison_Matrix.loc[n,'Test-MSE']  = round(mean_squared_error(Test_Y, pred2),20)\n",
    "    Model_Evaluation_Comparison_Matrix.loc[n,'Train-RMSE']= round(np.sqrt(mean_squared_error(Train_Y, pred1)),20)\n",
    "    Model_Evaluation_Comparison_Matrix.loc[n,'Test-RMSE'] = round(np.sqrt(mean_squared_error(Test_Y, pred2)),20)\n",
    "\n",
    "    # Plotting y_test and y_pred to understand the spread.\n",
    "    plt.figure(figsize=[15,4])\n",
    "\n",
    "    plt.subplot(1,2,1)\n",
    "    sns.distplot((Train_Y - pred1))\n",
    "    plt.title('Error Terms')          \n",
    "    plt.xlabel('Errors') \n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.scatter(Train_Y,pred1)\n",
    "    plt.plot([Train_Y.min(),Train_Y.max()],[Train_Y.min(),Train_Y.max()], 'r--')\n",
    "    plt.title('Test vs Prediction')         \n",
    "    plt.xlabel('y_test')                       \n",
    "    plt.ylabel('y_pred')                       \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objective:** Let us now try building multiple regression models & compare their evaluation metrics to choose the best fit model both training and testing sets..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6a. Multiple Linear Regression(MLR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-01T16:35:19.088652Z",
     "iopub.status.busy": "2022-01-01T16:35:19.087967Z",
     "iopub.status.idle": "2022-01-01T16:35:27.749857Z",
     "shell.execute_reply": "2022-01-01T16:35:27.74896Z",
     "shell.execute_reply.started": "2022-01-01T16:35:19.088603Z"
    }
   },
   "outputs": [],
   "source": [
    "#Linear Regression\n",
    "\n",
    "MLR = LinearRegression().fit(Train_X_std,Train_Y)\n",
    "pred1 = MLR.predict(Train_X_std)\n",
    "pred2 = MLR.predict(Test_X_std)\n",
    "\n",
    "print('{}{}\\033[1m Evaluating Multiple Linear Regression Model \\033[0m{}{}\\n'.format('<'*3,'-'*25 ,'-'*25,'>'*3))\n",
    "print('The Coeffecient of the Regresion Model was found to be ',MLR.coef_)\n",
    "print('The Intercept of the Regresion Model was found to be ',MLR.intercept_)\n",
    "\n",
    "Evaluate(0, pred1, pred2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6b. Ridge Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-01T16:35:27.751582Z",
     "iopub.status.busy": "2022-01-01T16:35:27.751248Z",
     "iopub.status.idle": "2022-01-01T16:35:36.658792Z",
     "shell.execute_reply": "2022-01-01T16:35:36.657999Z",
     "shell.execute_reply.started": "2022-01-01T16:35:27.751535Z"
    }
   },
   "outputs": [],
   "source": [
    "#Creating a Ridge Regression model\n",
    "\n",
    "RLR = Ridge().fit(Train_X_std,Train_Y)\n",
    "pred1 = RLR.predict(Train_X_std)\n",
    "pred2 = RLR.predict(Test_X_std)\n",
    "\n",
    "print('{}{}\\033[1m Evaluating Ridge Regression Model \\033[0m{}{}\\n'.format('<'*3,'-'*25 ,'-'*25,'>'*3))\n",
    "print('The Coeffecient of the Regresion Model was found to be ',MLR.coef_)\n",
    "print('The Intercept of the Regresion Model was found to be ',MLR.intercept_)\n",
    "\n",
    "Evaluate(1, pred1, pred2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6c. Lasso Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-01T16:35:36.660206Z",
     "iopub.status.busy": "2022-01-01T16:35:36.660006Z",
     "iopub.status.idle": "2022-01-01T16:35:45.31406Z",
     "shell.execute_reply": "2022-01-01T16:35:45.313248Z",
     "shell.execute_reply.started": "2022-01-01T16:35:36.660181Z"
    }
   },
   "outputs": [],
   "source": [
    "#Creating a Ridge Regression model\n",
    "\n",
    "LLR = Lasso().fit(Train_X_std,Train_Y)\n",
    "pred1 = LLR.predict(Train_X_std)\n",
    "pred2 = LLR.predict(Test_X_std)\n",
    "\n",
    "print('{}{}\\033[1m Evaluating Lasso Regression Model \\033[0m{}{}\\n'.format('<'*3,'-'*25 ,'-'*25,'>'*3))\n",
    "print('The Coeffecient of the Regresion Model was found to be ',MLR.coef_)\n",
    "print('The Intercept of the Regresion Model was found to be ',MLR.intercept_)\n",
    "\n",
    "Evaluate(2, pred1, pred2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6d. Elastic-Net Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-01T16:35:45.315412Z",
     "iopub.status.busy": "2022-01-01T16:35:45.315184Z",
     "iopub.status.idle": "2022-01-01T16:35:53.943951Z",
     "shell.execute_reply": "2022-01-01T16:35:53.943125Z",
     "shell.execute_reply.started": "2022-01-01T16:35:45.315386Z"
    }
   },
   "outputs": [],
   "source": [
    "#Creating a ElasticNet Regression model\n",
    "\n",
    "ENR = ElasticNet().fit(Train_X_std,Train_Y)\n",
    "pred1 = ENR.predict(Train_X_std)\n",
    "pred2 = ENR.predict(Test_X_std)\n",
    "\n",
    "print('{}{}\\033[1m Evaluating Elastic-Net Regression Model \\033[0m{}{}\\n'.format('<'*3,'-'*25 ,'-'*25,'>'*3))\n",
    "print('The Coeffecient of the Regresion Model was found to be ',MLR.coef_)\n",
    "print('The Intercept of the Regresion Model was found to be ',MLR.intercept_)\n",
    "\n",
    "Evaluate(3, pred1, pred2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6e. Polynomial Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-01T16:35:53.94578Z",
     "iopub.status.busy": "2022-01-01T16:35:53.945511Z",
     "iopub.status.idle": "2022-01-01T16:37:39.200415Z",
     "shell.execute_reply": "2022-01-01T16:37:39.199574Z",
     "shell.execute_reply.started": "2022-01-01T16:35:53.945751Z"
    }
   },
   "outputs": [],
   "source": [
    "#Checking polynomial regression performance on various degrees\n",
    "\n",
    "Trr=[]; Tss=[]\n",
    "n_degree=6\n",
    "\n",
    "for i in range(2,n_degree):\n",
    "    #print(f'{i} Degree')\n",
    "    poly_reg = PolynomialFeatures(degree=i)\n",
    "    X_poly = poly_reg.fit_transform(Train_X_std)\n",
    "    X_poly1 = poly_reg.fit_transform(Test_X_std)\n",
    "    LR = LinearRegression()\n",
    "    LR.fit(X_poly, Train_Y)\n",
    "    \n",
    "    pred1 = LR.predict(X_poly)\n",
    "    Trr.append(np.sqrt(mean_squared_error(Train_Y, pred1)))\n",
    "    \n",
    "    pred2 = LR.predict(X_poly1)\n",
    "    Tss.append(np.sqrt(mean_squared_error(Test_Y, pred2)))\n",
    "\n",
    "plt.figure(figsize=[15,6])\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(range(2,n_degree),Trr, label='Training')\n",
    "plt.plot(range(2,n_degree),Tss, label='Testing')\n",
    "#plt.plot([1,4],[1,4],'b--')\n",
    "plt.title('Polynomial Regression Fit')\n",
    "#plt.ylim([0,5])\n",
    "plt.xlabel('Degree')\n",
    "plt.ylabel('RMSE')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "#plt.xticks()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(range(2,n_degree),Trr, label='Training')\n",
    "plt.plot(range(2,n_degree),Tss, label='Testing')\n",
    "plt.title('Polynomial Regression Fit')\n",
    "plt.ylim([3,4.1])\n",
    "plt.xlabel('Degree')\n",
    "plt.ylabel('RMSE')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "#plt.xticks()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inference:** We can choose 5th order polynomial regression as it gives the optimal training & testing scores..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-01T16:37:39.202585Z",
     "iopub.status.busy": "2022-01-01T16:37:39.202251Z",
     "iopub.status.idle": "2022-01-01T16:39:15.861417Z",
     "shell.execute_reply": "2022-01-01T16:39:15.860497Z",
     "shell.execute_reply.started": "2022-01-01T16:37:39.202542Z"
    }
   },
   "outputs": [],
   "source": [
    "#Using the 5th Order Polynomial Regression model (degree=5)\n",
    "\n",
    "poly_reg = PolynomialFeatures(degree=5)\n",
    "X_poly = poly_reg.fit_transform(Train_X_std)\n",
    "X_poly1 = poly_reg.fit_transform(Test_X_std)\n",
    "PR = LinearRegression()\n",
    "PR.fit(X_poly, Train_Y)\n",
    "\n",
    "pred1 = PR.predict(X_poly)\n",
    "pred2 = PR.predict(X_poly1)\n",
    "\n",
    "print('{}{}\\033[1m Evaluating Polynomial Regression Model \\033[0m{}{}\\n'.format('<'*3,'-'*25 ,'-'*25,'>'*3))\n",
    "print('The Coeffecient of the Regresion Model was found to be ',MLR.coef_)\n",
    "print('The Intercept of the Regresion Model was found to be ',MLR.intercept_)\n",
    "\n",
    "Evaluate(4, pred1, pred2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
